{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1pg6ZvrF7qCACA3/p3mpo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saima-Hassan/Generative-AI/blob/main/Text_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24yQDcTghOCw",
        "outputId": "0822670f-cdd7-419a-cfb9-e879c43f37a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sent_tokenize: Splits a given text/paragraph into individual sentences.\n",
        "#word_tokenize: Splits a given text into individual words and punctuation.\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "n3V74QuqiACh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentence tokenization** : split a paragraph into list of sentences using sent_tokenize() method"
      ],
      "metadata": {
        "id": "_EaEG7Xmi85p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The goal of this Generative AI training course is to help participants understand generative AI, find and use existing models, train them effectively. And develop impactful applications tailored to their needs. As a result, participants can make money through freelancing, contracting, or secure a full-time job in generative AI.\"\n",
        "#STEP 1 :TOKENIZATION : Breaking complex data into simple units\n",
        "#Sentence Tokenizer\n",
        "sentences= sent_tokenize(text)\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9OblVC_kOLs",
        "outputId": "e23b0230-fda4-4e7d-bd49-9c3f7bfab9cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The goal of this Generative AI training course is to help participants understand generative AI, find and use existing models, train them effectively.\n",
            "\n",
            "And develop impactful applications tailored to their needs.\n",
            "\n",
            "As a result, participants can make money through freelancing, contracting, or secure a full-time job in generative AI.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word tokenization** : split a sentence into list of words using word_tokenize() method"
      ],
      "metadata": {
        "id": "_KjKmgJIk2LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Tokenizer\n",
        "for sentence in sentences:\n",
        "    words=word_tokenize(sentence)\n",
        "    print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hddK3jaslUaO",
        "outputId": "bc2c9b44-f244-46b7-b8e4-6e21844930ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'goal', 'of', 'this', 'Generative', 'AI', 'training', 'course', 'is', 'to', 'help', 'participants', 'understand', 'generative', 'AI', ',', 'find', 'and', 'use', 'existing', 'models', ',', 'train', 'them', 'effectively', '.']\n",
            "['And', 'develop', 'impactful', 'applications', 'tailored', 'to', 'their', 'needs', '.']\n",
            "['As', 'a', 'result', ',', 'participants', 'can', 'make', 'money', 'through', 'freelancing', ',', 'contracting', ',', 'or', 'secure', 'a', 'full-time', 'job', 'in', 'generative', 'AI', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming and Lemmatization**\n",
        "Stemming is a technique used to reduce an inflected word down to its word stem.\n",
        "Lemmatization is another technique used to reduce inflected words to their root word. It describes the algorithmic process of identifying an inflected word’s “lemma” (dictionary form) based on its intended meaning."
      ],
      "metadata": {
        "id": "7YrAbGwaldV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WordNet is a lexical database for the English language is part of the NLTK corpus. WordNet NLTK module to find the meanings of words, synonyms, antonyms, and more.\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zDneh_Sl0-E",
        "outputId": "705a6fa4-9bff-42cb-8ff2-0ad4539c428c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Example text\n",
        "text = \"The striped bats are hanging on their feet for best\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Apply stemming to each word\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Stemmed words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D658WbxXmiVG",
        "outputId": "b428219e-0481-4be4-92ec-a0b9fa12a39b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
            "Stemmed words: ['the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'feet', 'for', 'best']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Example text\n",
        "text = \"The striped bats are hanging on their feet for best\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply lemmatization to each word\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tumOmv_IoecT",
        "outputId": "6209f580-151f-42d4-8e68-30a586064d7b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
            "Lemmatized words: ['The', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'for', 'best']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopwords** are common words in a language that are often filtered out before processing natural language data because they carry less meaningful information compared to other words. Examples of stopwords in English include \"is,\" \"and,\" \"the,\" \"in,\" etc."
      ],
      "metadata": {
        "id": "KjmrTuHco6bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "print(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJYZBQFSo5Ei",
        "outputId": "92e5927c-4bcf-41c7-fbea-7d27e6416ef6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    }
  ]
}
